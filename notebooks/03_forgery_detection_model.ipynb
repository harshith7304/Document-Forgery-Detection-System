{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3:** Forgery Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"/content/indian-document-forgery-detection/dataset\"\n",
    "\n",
    "# Create the base directory\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "# Define class names\n",
    "class_names = [\"Real\", \"Forged\", \"Mask\"]\n",
    "\n",
    "# Loop through the class names and create subdirectories\n",
    "for class_name in class_names:\n",
    "    class_dir = os.path.join(base_dir, class_name)\n",
    "    os.makedirs(class_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def gen(field_coords,data,last,index,img, image_name):\n",
    "    length = len(field_coords)\n",
    "    for i in range(length):\n",
    "\n",
    "        data[index] = field_coords[i]  # Store the field and coordinates\n",
    "\n",
    "        if index == last:  # If we have a complete permutation\n",
    "            new_img = img.copy()\n",
    "            forged_mask = np.zeros_like(img[:, :, 0])\n",
    "\n",
    "            forged_name = f\"{image_name[:-4]}_{'_'.join([f'{field_name[0]}' for i, field_name in enumerate(data)])}.jpg\"\n",
    "\n",
    "\n",
    "            for i, field_name in enumerate(data):\n",
    "\n",
    "              for j, (other_field_name, other_coords) in enumerate(field_coords):\n",
    "                if i == j:\n",
    "                  dest_coords = other_coords\n",
    "\n",
    "              for j,(other_field_name, other_coords) in enumerate(field_coords):\n",
    "\n",
    "                if (field_name[0]==other_field_name and j!=i):\n",
    "                  forged_mask[dest_coords[0]['y']:dest_coords[0]['y'] + dest_coords[0]['height'],\n",
    "                            dest_coords[0]['x']:dest_coords[0]['x'] + dest_coords[0]['width']] = 255\n",
    "\n",
    "                  forged_mask_3_channel = cv2.cvtColor(forged_mask, cv2.COLOR_GRAY2BGR)  # Convert to BGR if needed\n",
    "                  mask_image_save_path = os.path.join(base_dir, \"Mask\", forged_name)\n",
    "                  cv2.imwrite(mask_image_save_path, forged_mask_3_channel)\n",
    "\n",
    "\n",
    "            for i, field_name in enumerate(data):\n",
    "\n",
    "              for j,(other_field_name, other_coords) in enumerate(field_coords):\n",
    "                if i==j:\n",
    "                  dest_coords=other_coords\n",
    "\n",
    "              for j,(other_field_name, other_coords) in enumerate(field_coords):\n",
    "\n",
    "                if (field_name[0]==other_field_name and j!=i):\n",
    "\n",
    "\n",
    "                  src_region = img[other_coords[0]['y']:other_coords[0]['y'] + other_coords[0]['height'],other_coords[0]['x']:other_coords[0]['x'] + other_coords[0]['width']]\n",
    "\n",
    "                  src_region_resized = cv2.resize(src_region,\n",
    "                                                (dest_coords[0]['width'], dest_coords[0]['height']))\n",
    "                  new_img[dest_coords[0]['y']:dest_coords[0]['y'] + dest_coords[0]['height'],\n",
    "                  dest_coords[0]['x']:dest_coords[0]['x'] + dest_coords[0]['width']] = src_region_resized\n",
    "                  forged_image_save_path = os.path.join(base_dir, \"Forged\", forged_name)\n",
    "\n",
    "                  cv2.imwrite(forged_image_save_path, new_img)\n",
    "                  # Upload to GitHub\n",
    "\n",
    "                  break\n",
    "\n",
    "                else:\n",
    "                  continue\n",
    "\n",
    "\n",
    "        else:\n",
    "            gen(field_coords, data, last, index + 1, img, image_name)\n",
    "\n",
    "\n",
    "\n",
    "def generate():\n",
    "    for i in range(100):\n",
    "        if(i<10):\n",
    "            image=f'0{i}.jpg'\n",
    "\n",
    "        else:\n",
    "            image=f'{i}.jpg'\n",
    "\n",
    "\n",
    "        image_Path = repository_path + image\n",
    "        img = cv2.imread(image_Path)\n",
    "\n",
    "        real_image_save_path = os.path.join(base_dir, \"Real\", image)\n",
    "        shutil.copy(image_Path, real_image_save_path)\n",
    "\n",
    "        with open(json_path) as f:\n",
    "          data = json.load(f);\n",
    "\n",
    "        # Extract metadata paths\n",
    "        region = [data['_via_img_metadata'][key]['regions']\n",
    "                  for key in data['_via_img_metadata']]\n",
    "        reg = region[i]\n",
    "\n",
    "        birth_date_coords = []\n",
    "        expiry_date_coords = []\n",
    "        issue_date_coords = []\n",
    "\n",
    "\n",
    "\n",
    "        surname_coords = []\n",
    "        name_coords = []\n",
    "\n",
    "\n",
    "\n",
    "        nationality_coords = []\n",
    "        birth_place_coords = []\n",
    "\n",
    "\n",
    "        number_coords = []\n",
    "        id_number_coords = []\n",
    "\n",
    "\n",
    "\n",
    "        for region in reg:\n",
    "          if region['region_attributes']['field_name'] == 'birth_date':\n",
    "            birth_date_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'expiry_date':\n",
    "            expiry_date_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'issue_date':\n",
    "            issue_date_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'surname':\n",
    "            surname_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'name':\n",
    "            name_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'nationality':\n",
    "            nationality_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'birth_place':\n",
    "            birth_place_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'number':\n",
    "            number_coords.append(region['shape_attributes'])\n",
    "          elif region['region_attributes']['field_name'] == 'id_number':\n",
    "            id_number_coords.append(region['shape_attributes'])\n",
    "        group1=[('birth_date', birth_date_coords), ('expiry_date', expiry_date_coords), ('issue_date', issue_date_coords)]\n",
    "        group2=[('surname', surname_coords), ('name', name_coords)]\n",
    "        group3=[('nationality', nationality_coords), ('birth_place', birth_place_coords)]\n",
    "        group4=[('number', number_coords), ('id_number', id_number_coords)]\n",
    "        groups = [\n",
    "            group1,\n",
    "            group2,\n",
    "            group3,\n",
    "            group4\n",
    "        ]\n",
    "\n",
    "        # Iterate through each group\n",
    "        # Iterate through each group\n",
    "        for group in groups:\n",
    "            if len(group) < 2:  # Skip groups with less than 2 fields\n",
    "                continue\n",
    "\n",
    "            # Get field names from the group\n",
    "            field_names = [item[0] for item in group]\n",
    "            all_coords = [item[1] for item in group]\n",
    "            length=len(group)\n",
    "            data=[None] * length\n",
    "\n",
    "            gen(group, data, length-1,0,img,image)\n",
    "\n",
    "\n",
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Count the data instants\n",
    "base_dir = \"/content/indian-document-forgery-detection/dataset\"\n",
    "forged_path = \"/content/indian-document-forgery-detection/dataset/Forged\"\n",
    "real_path = \"/content/indian-document-forgery-detection/dataset/Real\"\n",
    "def count_images_in_category(category_dir):\n",
    "    return len(os.listdir(category_dir))\n",
    "\n",
    "##Looping over the classes for training category\n",
    "# for class_name in class_names:\n",
    "#     train_category= os.path.join(base_dir, class_name)\n",
    "count= count_images_in_category(real_path)\n",
    "print(f\"Number of images in {class_name} training category: {count}\")\n",
    "count= count_images_in_category(forged_path)\n",
    "print(f\"Number of images in {class_name} training category: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function to copy a specified number of random samples from source to destination\n",
    "def copy_random_samples(src_dir, dest_dir, samples):\n",
    "    files = random.sample(os.listdir(src_dir), samples)\n",
    "    for file in files:\n",
    "        src_path = os.path.join(src_dir, file)\n",
    "        dest_path = os.path.join(dest_dir, file)\n",
    "        shutil.copy(src_path, dest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_path = '/content/indian-document-forgery-detection/dataset'\n",
    "# Destination paths\n",
    "train_path = os.path.join(original_path, 'train')\n",
    "val_path = os.path.join(original_path, 'val')\n",
    "test_path = os.path.join(original_path, 'test')\n",
    "\n",
    "# Create directories\n",
    "for path in [train_path, val_path, test_path]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "## DEFINE CLASSES AND SAMPLE SIZES ##\n",
    "## Define classes in the dataset\n",
    "classes = [\"Forged\", \"Real\"]\n",
    "\n",
    "## Set the number of samples for training, validation, and testing\n",
    "\n",
    "sample_train = [2450,70]\n",
    "sample_val = [700,20]\n",
    "sample_test = [350,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path= \"/content/indian-document-forgery-detection/dataset/test\"\n",
    "train_path= \"/content/indian-document-forgery-detection/dataset/train/im\"\n",
    "val_path= \"/content/indian-document-forgery-detection/dataset/val/im\"\n",
    "\n",
    "## FUNCTION TO COPY RANDOM SAMPLES ##\n",
    "\n",
    "## COPY SAMPLES FOR TRAINING ##\n",
    "for i, class_name in enumerate(classes):\n",
    "    src_class_dir = os.path.join(original_path, class_name)\n",
    "    dest_class_dir = os.path.join(train_path, class_name)\n",
    "    os.makedirs(dest_class_dir, exist_ok=True)\n",
    "\n",
    "    # Get the sample number for the current class\n",
    "    samples1 = sample_train[i]\n",
    "\n",
    "    # Copy the random samples\n",
    "    copy_random_samples(src_class_dir, dest_class_dir, samples1)\n",
    "\n",
    "## COPY SAMPLES FOR VALIDATION ##\n",
    "\n",
    "## Iterate over classes to copy random samples for validation\n",
    "for i, class_name in enumerate(classes):\n",
    "    src_class_dir = os.path.join(original_path, class_name)\n",
    "    dest_class_dir = os.path.join(val_path, class_name)\n",
    "    os.makedirs(dest_class_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "    # Get the sample number for the current class\n",
    "    samples2 = sample_val[i]\n",
    "\n",
    "    # Copy the random samples\n",
    "    copy_random_samples(src_class_dir, dest_class_dir, samples2)\n",
    "\n",
    "\n",
    "## COPY SAMPLES FOR TESTING ##\n",
    "## Iterate over classes to copy random samples for testing\n",
    "for i, class_name in enumerate(classes):\n",
    "    src_class_dir = os.path.join(original_path, class_name)\n",
    "    dest_class_dir = os.path.join(test_path, class_name)\n",
    "    os.makedirs(dest_class_dir, exist_ok=True)\n",
    "    # Get the sample number for the current class\n",
    "    samples3 = sample_test[i]\n",
    "\n",
    "    # Copy the random samples\n",
    "    copy_random_samples(src_class_dir, dest_class_dir, samples3)\n",
    "\n",
    "\n",
    "\n",
    "## Defining a function to count the data instants\n",
    "def count_images_in_category(category_dir):\n",
    "    return len(os.listdir(category_dir))\n",
    "\n",
    "##Looping over the classes for training category\n",
    "for class_name in classes:\n",
    "    train_category= os.path.join(train_path, class_name)\n",
    "    count= count_images_in_category(train_category)\n",
    "    print(f\"Number of images in {class_name} training category: {count}\")\n",
    "\n",
    "\n",
    "for class_name in classes:\n",
    "    test_category= os.path.join(test_path, class_name)\n",
    "    count = count_images_in_category(test_category)\n",
    "    print(f\"Number of images in {class_name} category (Testing): {count}\")\n",
    "\n",
    "\n",
    "\n",
    "for class_name in classes:\n",
    "    val_category= os.path.join(val_path, class_name)\n",
    "    count = count_images_in_category(val_category)\n",
    "    print(f\"Number of images in {class_name} category (Validation): {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_height = 256\n",
    "img_width = 256\n",
    "\n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "image_datagen_train = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "image_generator_train = image_datagen_train.flow_from_directory(\n",
    "    '/content/indian-document-forgery-detection/dataset/train/im',\n",
    "    class_mode='binary',\n",
    "    color_mode='rgb',\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=2)\n",
    "\n",
    "image_datagen_valid = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "image_generator_valid = image_datagen_valid.flow_from_directory(\n",
    "    '/content/indian-document-forgery-detection/dataset/val/im',\n",
    "    class_mode=\"binary\",\n",
    "    color_mode='rgb',\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=2)\n",
    "\n",
    "train_generator = image_generator_train\n",
    "validation_generator = image_generator_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = Sequential()\n",
    "\n",
    "pretrained_model= tf.keras.applications.ResNet50(include_top=False,\n",
    "                   input_shape=(img_height,img_width,3),\n",
    "                   pooling='avg',classes=2,\n",
    "                   weights='imagenet')\n",
    "for layer in pretrained_model.layers: # dont learn weights again just use the exisiting one\n",
    "        layer.trainable=False\n",
    "\n",
    "resnet_model.add(pretrained_model)\n",
    "resnet_model.add(Flatten())\n",
    "resnet_model.add(Dense(512, activation='relu'))\n",
    "resnet_model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.Adam(learning_rate=0.001)\n",
    "resnet_model.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "# history = model.fit_generator(\n",
    "#         train_generator,\n",
    "#         epochs=nb_epochs,\n",
    "#         validation_data=validation_generator,\n",
    "#         verbose=1,\n",
    "#         steps_per_epoch=10//batch_size,\n",
    "#         validation_steps=10//batch_size,\n",
    "#         callbacks=cal_b)\n",
    "\n",
    "\n",
    "history = resnet_model.fit(\n",
    "  train_generator,\n",
    "  validation_data=validation_generator,\n",
    "  epochs=epochs,\n",
    "  verbose=1,\n",
    "        steps_per_epoch=20//batch_size,\n",
    "        validation_steps=10//batch_size,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen_test = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "\n",
    "image_generator_test = image_datagen_test.flow_from_directory(\n",
    "    '/content/indian-document-forgery-detection/dataset/test',\n",
    "    class_mode=\"binary\",\n",
    "    color_mode='rgb',\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_generator =image_generator_test\n",
    "\n",
    "results = resnet_model.evaluate(test_generator)\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "y_pred = resnet_model.predict(test_generator)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_generator.classes\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Forged\", \"Real\"],\n",
    "            yticklabels=[\"Forged \", \"Real\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "for i in range(len(filenames)):\n",
    "    img_path = os.path.join(test_path, filenames[i])\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    prediction = resnet_model.predict(img_array)\n",
    "    predicted_class = int(np.round(prediction)[0])\n",
    "\n",
    "    true_class = int(y_true[i])\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True Class: {true_class}, Predicted Class: {predicted_class}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model number 2\n",
    "model_3 = Sequential([\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" , input_shape = (256,256,3),) , # 32 is the number of filters , (3,3) is the size of the filter , input shape is the size of images the models is expecting\n",
    "    MaxPooling2D((2,2)),\n",
    "\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" ),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" ),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" ),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" ),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Conv2D(32,(3,3) , activation = \"relu\" ),\n",
    "    MaxPooling2D((2,2)),\n",
    "    Flatten(),\n",
    "    Dense(512, activation = 'relu'), # 64 neurons\n",
    "    Dense(256, activation = 'relu'), # 64 neurons\n",
    "    Dropout(0.2),\n",
    "    Dense(1, activation = 'sigmoid'), # 2 output neuron\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3.compile(optimizer=\"adam\",loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=20\n",
    "# history = model.fit_generator(\n",
    "#         train_generator,\n",
    "#         epochs=nb_epochs,\n",
    "#         validation_data=validation_generator,\n",
    "#         verbose=1,\n",
    "#         steps_per_epoch=10//batch_size,\n",
    "#         validation_steps=10//batch_size,\n",
    "#         callbacks=cal_b)\n",
    "\n",
    "\n",
    "history = model_3.fit(\n",
    "  train_generator,\n",
    "  validation_data=validation_generator,\n",
    "  epochs=epochs,\n",
    "  verbose=1,\n",
    "  steps_per_epoch=20//batch_size,\n",
    "  validation_steps=10//batch_size,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datagen_test = keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "\n",
    "image_generator_test = image_datagen_test.flow_from_directory(\n",
    "    '/content/indian-document-forgery-detection/dataset/test',\n",
    "    class_mode=\"binary\",\n",
    "    color_mode='rgb',\n",
    "    target_size=(img_height,img_width),\n",
    "    batch_size=batch_size,\n",
    "    seed=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_generator =image_generator_test\n",
    "\n",
    "results = model_3.evaluate(test_generator)\n",
    "print(\"Test Loss:\", results[0])\n",
    "print(\"Test Accuracy:\", results[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_3.predict(test_generator)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)\n",
    "\n",
    "y_true = test_generator.classes\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"Forged\", \"Real\"],\n",
    "            yticklabels=[\"Forged \", \"Real\"])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "for i in range(len(filenames)):\n",
    "    img_path = os.path.join(test_path, filenames[i])\n",
    "    img = image.load_img(img_path, target_size=(256, 256))\n",
    "    img_array = image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array /= 255.0\n",
    "\n",
    "    prediction = model_3.predict(img_array)\n",
    "    predicted_class = int(np.round(prediction)[0])\n",
    "\n",
    "    true_class = int(y_true[i])\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"True Class: {true_class}, Predicted Class: {predicted_class}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
